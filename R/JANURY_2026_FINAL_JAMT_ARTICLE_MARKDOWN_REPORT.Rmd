---
title: Settlement patterns and community delimitation in Chibcha chiefdoms of Colombia and Panama
author: "Luis Miguel Soto Rodriguez"
date: "`r Sys.Date()`"
geometry: "margin=1cm"
output:
  html_document: default
  pdf_document: 
    latex_engine: xelatex
---
# 1. Environment Configuration
```{r}
# 
base_packages <- c(
  "sf", "terra", "gstat", "dplyr", "ggplot2", "patchwork",
  "dbscan", "scico", "viridis", "spdep", "igraph", "mclust",
  "purrr", "knitr", "kableExtra", "ggspatial", "car", "ggrepel", "stats", "boot",
  "vegan", "tidyr", "scales", "gdistance", "raster", "sp", "elevatr"
)

# 
nn_required <- c("spatstat.geom")
nn_optional <- c("spatstat.explore")
required_packages <- unique(c(base_packages, nn_required, nn_optional))

install_if_missing <- function(pkgs) {
  to_install <- pkgs[!sapply(pkgs, requireNamespace, quietly = TRUE)]
  if (length(to_install) == 0) return(invisible(TRUE))
  args <- list(pkgs = to_install, dependencies = TRUE)
  if (.Platform$OS.type == "windows") args$type <- "binary"
  do.call(install.packages, args)
}

install_if_missing(required_packages)

# 
load_safely <- function(pkgs) {
  invisible(lapply(pkgs, function(p) {
    ok <- suppressWarnings(suppressPackageStartupMessages(require(p, character.only = TRUE)))
    if (!ok) message(sprintf("Package '%s' is not available. Continuing without it.", p))
  }))
}

load_safely(unique(c(base_packages, nn_required)))
load_safely(nn_optional)

# 
if (!requireNamespace("spatstat.geom", quietly = TRUE)) {
  stop("Package 'spatstat.geom' is required for nearest-neighbor analysis (nndist).")
}

# 
base_theme_map <- theme_void() +
  theme(
    plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
    plot.subtitle = element_text(size = 11, hjust = 0.5, margin = margin(b = 10)),
    legend.title = element_text(size = 10), legend.text = element_text(size = 9),
    plot.margin = margin(12, 12, 12, 12)
  )
theme_set(theme_bw(base_size = 11))

# 
GITHUB_BASE <- "https://raw.githubusercontent.com/luismiguelsoto/Community_delimitation_models_and_settlement_patterns_in_Chibcha_chiefdoms/main/GIS"

# 
.rtrim_slash <- function(x) sub("/+$", "", x)

# 
.download_if_exists <- function(url, dest) {
  ok <- try(utils::download.file(url, dest, mode = "wb", quiet = TRUE), silent = TRUE)
  invisible(!inherits(ok, "try-error"))
}

# 
.read_github_shp <- function(base_url, shp_filename, dest_dir = file.path(tempdir(), "gh_shp_cache")) {
  dir.create(dest_dir, showWarnings = FALSE, recursive = TRUE)
  base_url <- .rtrim_slash(base_url)
  stem <- sub("\\.shp$", "", shp_filename, ignore.case = TRUE)
  exts <- c("shp", "shx", "dbf", "prj", "cpg")
  downloaded <- logical(length(exts))
  for (i in seq_along(exts)) {
    u <- sprintf("%s/%s.%s", base_url, stem, exts[i])
    d <- file.path(dest_dir, sprintf("%s.%s", stem, exts[i]))
    downloaded[i] <- .download_if_exists(u, d)
  }
  needed <- setNames(downloaded[match(c("shp","shx","dbf"), exts)], c("shp","shx","dbf"))
  if (all(needed, na.rm = TRUE)) {
    return(sf::st_read(file.path(dest_dir, paste0(stem, ".shp")), quiet = TRUE, stringsAsFactors = FALSE))
  } else {
    stop("Could not download a complete shapefile from GitHub: ", shp_filename)
  }
}

# 
REGIONES <- list(
  FUQUENE = list(
    sites    = "SITES_FUQUENE_LATE_MUISCA_UTM_polygons.shp",
    boundary = "POLYGON_FUQUENE_UTM_polygons.shp",
    name     = "Fuquene (Late Muisca)"
  ),
  PARITA = list(
    sites    = "SITES_PARITA_LATE_HATILLO_UTM_polygons.shp",
    boundary = "POLYGON_PARITA_UTM_polygons.shp",
    name     = "Parita (Late Hatillo)"
  ),
  RIO_FRIO = list(
    sites    = "SITES_RIO_FRIO_TAIRONA_PERIOD_UTM_polygons.shp",
    boundary = "POLYGON_RIO_FRIO_UTM_Polygons.shp",
    name     = "Rio Frio (Tairona)"
  ),
  TONOSI = list(
    sites    = "SITES_TONOSI_LATE_BIJAGUALES_UTM_polygons.shp",
    boundary = "POLYGON_TONOSI_UTM_polygons.shp",
    name     = "Tonosi (Bijaguales)"
  )
)

# 
CFG <- list(
  grid_res_m = 100,
  idw_power = 0.25,
  percolation_hours = seq(0.25, 6, 0.25),
  hdbscan_min_pts = 5,
  network_knn = 3,
  spatial_threshold_percentile = 25,
  min_community_area_ha = 3,
  bbox_extension_factor = 0.2,
  mobility_model = "tobler",
  slope_weight  = 0.6,
  rough_weight  = 0.4,
  landcover_raster = NULL
)

# 
to_numeric_safely <- function(x) {
  if (is.numeric(x)) return(x)
  x <- gsub("[^0-9eE+\\-\\.,]", "", as.character(x))
  x <- sub("^([0-9]+),([0-9]+)$", "\\1.\\2", x)
  suppressWarnings(as.numeric(x))
}

# 
compute_area_ha <- function(sites_sf) {
  nm <- names(sites_sf)
  idx1 <- which(grepl("^size_?ha$|_ha$|\\bha\\b", nm, ignore.case = TRUE))[1]
  idx2 <- which(grepl("shape_?area", nm, ignore.case = TRUE))[1]
  idx3 <- which(grepl("area|size", nm, ignore.case = TRUE))[1]
  if (!is.na(idx1)) return(to_numeric_safely(sites_sf[[idx1]]))
  if (!is.na(idx2)) return(to_numeric_safely(sites_sf[[idx2]]) / 10000)
  if (!is.na(idx3)) {
    v <- to_numeric_safely(sites_sf[[idx3]])
    med <- suppressWarnings(median(v, na.rm = TRUE))
    if (is.finite(med) && med > 5e3) v <- v / 10000
    return(v)
  }
  as.numeric(st_area(sites_sf)) / 10000
}

# 
flood_fill_simple <- function(labels, mask, i, j, label) {
  nr <- nrow(mask); nc <- ncol(mask)
  stack <- list(c(i, j))
  while (length(stack) > 0) {
    current <- stack[[length(stack)]]; stack <- stack[-length(stack)]
    ci <- current[1]; cj <- current[2]
    if (ci < 1 || ci > nr || cj < 1 || cj > nc) next
    if (is.na(mask[ci, cj]) || mask[ci, cj] == 0 || labels[ci, cj] != 0) next
    labels[ci, cj] <- label
    stack <- c(stack, list(c(ci - 1, cj), c(ci + 1, cj), c(ci, cj - 1), c(ci, cj + 1)))
  }
  labels
}

# 
robust_crop_mask <- function(raster_in, boundary_sf) {
  tryCatch(
    {
      boundary_vect <- terra::vect(boundary_sf)
      boundary_extended <- terra::buffer(boundary_vect, width = 50)
      terra::mask(terra::crop(raster_in, boundary_extended, extend = TRUE), boundary_vect)
    },
    error = function(e) {
      tryCatch(
        {
          boundary_vect <- terra::vect(boundary_sf)
          terra::mask(terra::crop(raster_in, boundary_vect), boundary_vect)
        },
        error = function(e2) raster_in
      )
    }
  )
}

# 
SER <- function(x) sd(x, na.rm = TRUE) / sqrt(length(na.omit(x)))

# 
ari_safe <- function(a, b) {
  idx <- !is.na(a) & !is.na(b)
  if (sum(idx) >= 3) mclust::adjustedRandIndex(a[idx], b[idx]) else NA_real_
}

# 
deg2rad <- function(x) x * pi / 180

# 
tobler_speed_kmh <- function(slope_deg) {
  s <- tan(deg2rad(slope_deg))
  v <- 6 * exp(-3.5 * abs(s + 0.05))
  pmax(v, 0.1)
}

# 
load_dem_region <- function(boundary_sf, crs_target, dem_path = NULL) {
  if (!is.null(dem_path) && file.exists(dem_path)) {
    dem <- raster::raster(dem_path)
  } else {
    bb <- st_bbox(boundary_sf)
    pts <- data.frame(x = c(bb["xmin"], bb["xmax"]), y = c(bb["ymin"], bb["ymax"]))
    sp_pts <- sp::SpatialPoints(pts, proj4string = sp::CRS(st_crs(boundary_sf)$wkt))
    dem <- suppressMessages(elevatr::get_elev_raster(sp_pts, z = 10, clip = "bbox"))
  }
  if (!is.na(crs_target$wkt))
    dem <- raster::projectRaster(dem, crs = crs_target$wkt, method = "bilinear")
  b_poly <- sf::st_buffer(boundary_sf, dist = 200) |> sf::st_transform(crs_target)
  dem <- raster::crop(dem, as(raster::extent(sf::st_bbox(b_poly)), "SpatialPolygons"))
  dem <- raster::mask(dem, as(b_poly, "Spatial"))
  dem
}

# 
transition_from_dem <- function(dem) {
  slope_deg <- raster::terrain(dem, opt = "slope", unit = "degrees")
  speed_kmh <- raster::calc(slope_deg, fun = tobler_speed_kmh)
  speed_mps <- raster::calc(speed_kmh, fun = function(x) x * 1000 / 3600)
  tr <- gdistance::transition(speed_mps, function(x) mean(x, na.rm = TRUE), directions = 8)
  tr <- gdistance::geoCorrection(tr, type = "c")
  tr
}

# 
time_matrix_hours <- function(coords_xy, tr, crs_tr) {
  if (nrow(coords_xy) < 2) return(matrix(0, nrow(coords_xy), nrow(coords_xy)))
  sp_pts <- sp::SpatialPoints(coords_xy, proj4string = sp::CRS(crs_tr$wkt))
  t_sec <- as.matrix(gdistance::costDistance(tr, sp_pts))
  t_sec[is.infinite(t_sec)] <- NA
  t_sec / 3600
}

# 
scale01 <- function(x) {
  xr <- range(values(x), na.rm = TRUE)
  if (!is.finite(xr[1]) || xr[1] == xr[2]) return(x*0)
  (x - xr[1]) / (xr[2] - xr[1])
}

# 
.safe_resample <- function(x, template) {
  tryCatch(terra::resample(x, template, method = "bilinear"), error = function(e) x)
}

# 
transition_from_dem_custom <- function(dem, model = "tobler",
                                       slope_w = 0.6, rough_w = 0.4,
                                       landcover_path = NULL) {
  dem <- tryCatch(terra::rast(dem), error = function(e) dem)
  slope_rad <- terra::terrain(dem, "slope", unit = "radians")
  slope_deg <- terra::terrain(dem, "slope", unit = "degrees")
  rough     <- terra::terrain(dem, "roughness")
  if (!is.null(landcover_path) && file.exists(landcover_path)) {
    lc_raw <- terra::rast(landcover_path)
    lc_raw <- .safe_resample(lc_raw, dem)
    lc_resist <- lc_raw; lc_resist[] <- NA
    lc_resist[lc_raw == 1] <- 0.9
    lc_resist[lc_raw == 2] <- 0.8
    lc_resist[lc_raw == 3] <- 0.5
    lc_resist[lc_raw == 4] <- 0.3
    lc_resist[lc_raw == 5] <- 0.7
    lc_resist[lc_raw == 6] <- 0.2
    lc_resist <- scale01(lc_resist)
  } else {
    lc_resist <- dem*0
  }
  if (model == "tobler") {
    v_kmh <- 6 * exp(-3.5 * abs(tan(slope_rad) + 0.05))
    v_mps <- v_kmh * 1000/3600
    cost  <- 1 / pmax(v_mps, 1e-4)
  } else if (model == "minetti") {
    v_kmh <- 1.77 * exp(-0.35 * (tan(slope_rad) + 0.05)^2)
    v_mps <- v_kmh * 1000/3600
    cost  <- 1 / pmax(v_mps, 1e-4)
  } else if (model == "naismith") {
    up    <- (slope_deg > 0) * slope_deg
    v_kmh <- 5 / (1 + up * 0.10)
    v_mps <- v_kmh * 1000/3600
    cost  <- 1 / pmax(v_mps, 1e-4)
  } else if (model == "energy") {
    E <- 1.5 + 0.35 * (tan(slope_rad))^2
    cost <- scale01(E) + 1e-6
  } else if (model == "tobler") {
    slope_term <- scale01(abs(tan(slope_rad)))
    rough_term <- scale01(rough)
    tw <- slope_w + rough_w
    if (tw > 1) { slope_w <- slope_w/tw; rough_w <- rough_w/tw }
    cost <- 1 + slope_w * slope_term + rough_w * rough_term + lc_resist
  } else {
    stop("Unknown mobility model: ", model)
  }
  conduct   <- 1 / pmax(cost, 1e-6)
  conduct_r <- raster::raster(conduct)
  tr <- gdistance::transition(conduct_r, transitionFunction = mean, directions = 8)
  tr <- gdistance::geoCorrection(tr, type = "c")
  tr
}

# 
tbl_kable <- function(df, caption, label, ...) {
  knitr::kable(df, caption = paste0(caption, " (\\#tab:", label, ")"), ...)
}
```

# 2. Data Loading and Preprocessing
```{r}
# 
all_data <- list()

# 
.read_layer_any <- function(filename) {
  tryCatch(
    .read_github_shp(GITHUB_BASE, filename),
    error = function(e) {
      stop(
        "No se pudo leer '", filename,
        "' desde GitHub: ", conditionMessage(e),
        "\nVerifica conexión o que el archivo exista en el repo:\n  ", GITHUB_BASE, "/", filename
      )
    }
  )
}

# 
for (region_name in names(REGIONES)) {
  cat("Loading:", REGIONES[[region_name]]$name, "\n")
  region_info <- REGIONES[[region_name]]

  sites_raw    <- .read_layer_any(region_info$sites)
  boundary_raw <- .read_layer_any(region_info$boundary)

  if (is.na(st_crs(boundary_raw))) st_crs(boundary_raw) <- 32618
  target_crs <- st_crs(boundary_raw)
  if (is.na(st_crs(sites_raw))) st_crs(sites_raw) <- target_crs
  if (st_crs(sites_raw)$epsg != target_crs$epsg) sites_raw <- st_transform(sites_raw, target_crs)

  sites_raw    <- st_make_valid(sites_raw)
  boundary_raw <- st_make_valid(boundary_raw)
  boundary_plot <- st_transform(boundary_raw, target_crs) |> st_make_valid()

  sites <- sites_raw

  if (all(st_geometry_type(boundary_raw) %in% c("POLYGON", "MULTIPOLYGON"))) {
    boundary_mask <- st_union(boundary_raw) |> st_as_sf()
  } else if (all(st_geometry_type(boundary_raw) %in% c("LINESTRING", "MULTILINESTRING"))) {
    poly_sfc <- st_collection_extract(st_polygonize(st_geometry(boundary_raw)), "POLYGON")
    if (length(poly_sfc) == 0) {
      ch <- st_convex_hull(st_union(suppressWarnings(st_centroid(sites))))
      buffer_dist <- 1000
      boundary_mask <- st_buffer(ch, dist = buffer_dist) |> st_as_sf()
    } else {
      polys <- st_as_sf(poly_sfc)
      pts <- suppressWarnings(st_centroid(sites))
      counts <- lengths(st_intersects(polys, pts))
      idx <- if (any(counts > 0)) which.max(counts) else which.max(st_area(polys))
      boundary_mask <- st_geometry(polys[idx, ]) |> st_as_sf()
    }
  } else {
    ch <- st_convex_hull(st_union(suppressWarnings(st_centroid(sites))))
    buffer_dist <- 1000
    boundary_mask <- st_buffer(ch, dist = buffer_dist) |> st_as_sf()
  }

  sites_centroids <- suppressWarnings(st_centroid(sites))
  coords <- st_coordinates(sites_centroids)

  sites_df <- data.frame(site_id = seq_len(nrow(sites)), X = coords[, 1], Y = coords[, 2], region = region_name)

  area_ha <- compute_area_ha(sites) |> to_numeric_safely()

  if (all(is.na(area_ha))) area_ha <- as.numeric(st_area(sites)) / 10000
  med <- suppressWarnings(median(area_ha, na.rm = TRUE))
  if (!is.finite(med)) med <- 0.01
  area_ha[is.na(area_ha) | !is.finite(area_ha) | area_ha <= 0] <- max(0.01, med)
  sites_df$area_ha <- area_ha

  all_data[[region_name]] <- list(
    sites_polygons = sites,
    sites_centroids = sites_centroids,
    sites_df = sites_df,
    boundary_mask = boundary_mask,
    boundary_plot = boundary_plot,
    crs = target_crs,
    name = region_info$name,
    region_id = region_name
  )

  cat(" ✓ Sites (original):", nrow(sites), " | Total Area:", round(sum(sites_df$area_ha), 1), "ha\n")
}

# 
cat("\n✓ All regions loaded successfully\n\n")

# 2.1. Dataset Validation: Edge Effects, Morphology, and Topography Controls
dataset_validation_list <- list()
topo_control_list <- list()

for (region_name in names(all_data)) {
  data <- all_data[[region_name]]
  cat("Validating dataset for:", data$name, "\n")
  
  # --- A. EDGE EFFECT ANALYSIS ---
  # 1. Convert boundary to line to measure distance
  boundary_line <- st_cast(data$boundary_mask, "MULTILINESTRING")
  dist_to_edge  <- as.numeric(st_distance(data$sites_centroids, boundary_line))
  
  # 2. Statistical Estimation of Edge Bias
  # Define "Near Edge" as < 200m (buffer zone)
  n_sites <- nrow(data$sites_df)
  n_near_edge <- sum(dist_to_edge < 200)
  
  # Calculate 95% Confidence Interval for the proportion of sites near edge
  # using Exact Binomial Test
  binom_res <- binom.test(n_near_edge, n_sites, conf.level = 0.95)
  prop_near <- binom_res$estimate
  ci_lower  <- binom_res$conf.int[1]
  ci_upper  <- binom_res$conf.int[2]
  
  # --- B. CENTROID VALIDITY ---
  # Site radius assuming circularity
  site_radius <- sqrt(data$sites_df$area_ha * 10000 / pi) 
  
  # Nearest Neighbor Distance (NND)
  coords <- st_coordinates(data$sites_centroids)
  d_mat <- as.matrix(dist(coords)); diag(d_mat) <- NA
  nnd <- apply(d_mat, 1, min, na.rm=TRUE)
  
  # Ratio: Radius / NND (If > 0.5, centroids might overlap neighbors)
  risk_ratio <- site_radius / nnd
  mean_risk  <- mean(risk_ratio, na.rm=TRUE)
  
  # --- C. TOPOGRAPHIC CONTROL (Area vs Slope) ---
  # Robust DEM download and processing (sf/terra compatible)
  slope_vals <- rep(NA, n_sites)
  dem_ok <- FALSE
  
  try({
    # Download DEM using the SF object directly
    dem_raw <- elevatr::get_elev_raster(data$boundary_mask, z = 10, clip = "bbox", verbose = FALSE)
    # Convert to terra and reproject to UTM (meters) for accurate slope calc
    dem_terra <- terra::rast(dem_raw)
    if (terra::crs(dem_terra) != data$crs$wkt) {
      dem_terra <- terra::project(dem_terra, data$crs$wkt)
    }
    # Calculate slope
    slope_rast <- terra::terrain(dem_terra, "slope", unit = "degrees")
    # Zonal statistics: mean slope per site polygon
    # Ensure vectors are in same CRS
    site_vect <- terra::vect(data$sites_polygons)
    if (terra::crs(site_vect) != terra::crs(slope_rast)) {
      site_vect <- terra::project(site_vect, terra::crs(slope_rast))
    }
    extracted <- terra::extract(slope_rast, site_vect, fun = mean, na.rm = TRUE)
    slope_vals <- extracted[, 2] # 2nd column has values
    dem_ok <- TRUE
  }, silent = TRUE)
  
  # Spearman Correlation: Site Area vs. Slope
  if (dem_ok && sum(!is.na(slope_vals)) > 5) {
    cor_res <- cor.test(data$sites_df$area_ha, slope_vals, method = "spearman", exact=FALSE)
    rho_val <- cor_res$estimate
    p_val_slope <- cor_res$p.value
    avg_slope <- mean(slope_vals, na.rm=TRUE)
  } else {
    rho_val <- NA; p_val_slope <- NA; avg_slope <- NA
  }

  # --- D. MORPHOLOGY ---
  # CV of area to determine hierarchy
  cv_area <- sd(data$sites_df$area_ha) / mean(data$sites_df$area_ha)

  # Compile Results
  dataset_validation_list[[region_name]] <- data.frame(
    Region = data$name,
    N_Sites = n_sites,
    # Edge Stats
    Pct_Near_Edge = round(prop_near * 100, 1),
    Edge_CI_95 = paste0("[", round(ci_lower*100, 1), "-", round(ci_upper*100, 1), "]%"),
    # Centroid Stats
    Centroid_Risk_Ratio = round(mean_risk, 2),
    # Topo Stats
    Avg_Slope_Deg = round(avg_slope, 1),
    Area_Slope_Corr_Rho = round(rho_val, 3),
    Area_Slope_P_Val = round(p_val_slope, 3),
    # Hierarchy
    Area_CV = round(cv_area, 2)
  )
}

# Print Combined Table
validation_df <- do.call(rbind, dataset_validation_list)

cat("\n### Table. Dataset Validation: Edge Effects, Geometric Simplification, and Topographic Control\n")
print(knitr::kable(validation_df, row.names = FALSE, 
                   caption = "Validation metrics addressing edge effects (95% CI), centroid usage risks, and topographic determinism."))

cat("\n✓ Validation analysis complete.\n")
```

# 3. Mobility Cost Calculation
```{r}
mobility_results <- list()
for (region_name in names(all_data)) {
  data <- all_data[[region_name]]
  coords_xy <- as.matrix(data$sites_df[, c("X", "Y")])
  D_time_h <- NULL
  dem_used <- NA_character_
  if (nrow(coords_xy) >= 2) {
    ok <- FALSE
    dem_path <- tryCatch({ NULL }, error = function(e) NULL)
    try({
      dem <- load_dem_region(data$boundary_mask, data$crs, dem_path)
      tr  <- transition_from_dem_custom(
        dem,
        model = CFG$mobility_model,
        slope_w = CFG$slope_weight,
        rough_w = CFG$rough_weight,
        landcover_path = CFG$landcover_raster
      )
      D_time_h <- time_matrix_hours(coords_xy, tr, data$crs)
      dem_used <- if (!is.null(dem_path) && file.exists(dem_path)) basename(dem_path) else "elevatr_bbox"
      ok <- TRUE
    }, silent = TRUE)
    if (!ok || any(is.na(D_time_h))) {
      De <- as.matrix(dist(coords_xy))
      D_time_h <- De / 1000 / 5
      dem_used <- "fallback_euclid"
    }
  } else {
    D_time_h <- matrix(0, nrow(coords_xy), nrow(coords_xy))
    dem_used <- NA_character_
  }
  mobility_results[[region_name]] <- list(
    D_time_h = D_time_h,
    dem = dem_used,
    model = CFG$mobility_model
  )
}
```

# 4. Method 1: IDW (Manuscript Fig. 2)
```{r fig.height=11, fig.width=8.5, warning=FALSE}
# 
traditional_results <- list()
idw_summary_list <- list()
idw_plot_list <- list()

# 
plot_tags <- c(
  RIO_FRIO = "A",
  FUQUENE = "B",
  PARITA = "C",
  TONOSI = "D"
)

# 
for (i in seq_along(all_data)) {
  region_name <- names(all_data)[i]
  data <- all_data[[region_name]]
  cat("Processing IDW:", data$name, "\n")
  
  # 
  original_bbox <- st_bbox(data$boundary_mask)
  x_range <- original_bbox["xmax"] - original_bbox["xmin"]
  y_range <- original_bbox["ymax"] - original_bbox["ymin"]
  bbox <- original_bbox + c(
    -x_range * CFG$bbox_extension_factor, -y_range * CFG$bbox_extension_factor,
    x_range * CFG$bbox_extension_factor, y_range * CFG$bbox_extension_factor
  )
  
  # 
  grid_rast <- terra::rast(
    xmin = bbox["xmin"], xmax = bbox["xmax"],
    ymin = bbox["ymin"], ymax = bbox["ymax"],
    resolution = CFG$grid_res_m, crs = st_crs(data$boundary_mask)$wkt
  )
  
  # 
  grid_sf <- st_as_sf(as.data.frame(terra::crds(terra::as.points(grid_rast))), coords = c("x", "y"), crs = st_crs(data$boundary_mask))
  sites_sf <- st_as_sf(data$sites_df, coords = c("X", "Y"), crs = st_crs(data$boundary_mask))
  
  # 
  idw_result <- tryCatch({
    gstat::idw(area_ha ~ 1, locations = sites_sf, newdata = grid_sf, idp = CFG$idw_power)
  }, error = function(e) NULL)
  
  # 
  if (is.null(idw_result)) {
    cat("  IDW failed for", data$name, "\n")
    next
  }
  
  # 
  idw_sf <- st_as_sf(idw_result)
  idw_surface <- terra::rasterize(terra::vect(idw_sf), grid_rast, field = "var1.pred", fun = mean)
  
  # 
  boundary_vect <- terra::vect(data$boundary_mask)
  idw_surface <- tryCatch({
    terra::mask(terra::crop(idw_surface, boundary_vect, extend = FALSE), boundary_vect)
  }, error = function(e) idw_surface)
  
  # 
  vals <- values(idw_surface)
  vals_clean <- vals[!is.na(vals) & is.finite(vals)]
  
  # 
  if (length(vals_clean) == 0) {
    cat("  No valid IDW values\n")
    next
  }
  
  # 
  threshold <- quantile(vals_clean, probs = CFG$spatial_threshold_percentile / 100, na.rm = TRUE)
  
  # 
  mask_significant <- idw_surface >= threshold
  mat <- as.matrix(mask_significant, wide = TRUE)
  mat[is.na(mat)] <- 0
  
  # 
  patches <- mat * 0
  patch_id <- 1
  
  # 
  for (ri in 1:nrow(mat)) {
    for (cj in 1:ncol(mat)) {
      if (mat[ri, cj] == 1 && patches[ri, cj] == 0) {
        patches <- flood_fill_simple(patches, mat, ri, cj, patch_id)
        patch_id <- patch_id + 1
      }
    }
  }
  
  # 
  communities_rast <- terra::rast(idw_surface)
  values(communities_rast) <- as.vector(t(patches))
  
  # 
  cell_area_ha <- (CFG$grid_res_m^2) / 10000
  min_cells <- ceiling(CFG$min_community_area_ha / cell_area_ha)
  
  # 
  patch_freq <- terra::freq(communities_rast)
  patch_freq <- patch_freq[patch_freq$value > 0, , drop = FALSE]
  large_patches <- patch_freq[patch_freq$count >= min_cells, ]
  n_communities <- nrow(large_patches)
  
  # 
  site_assign <- terra::extract(communities_rast, terra::vect(data$sites_centroids))[, 2]
  
  # 
  traditional_results[[region_name]] <- list(
    surface = idw_surface,
    communities_raster = communities_rast,
    n_communities = n_communities,
    site_assignments = site_assign,
    threshold = threshold
  )
  
  # 
  df_r <- terra::as.data.frame(idw_surface, xy = TRUE, na.rm = TRUE)
  names(df_r)[3] <- "IDW"
  
  # 
  p_trad <- ggplot() +
    geom_raster(data = df_r, aes(x = x, y = y, fill = IDW), alpha = 0.85, show.legend = FALSE) +
    geom_sf(data = data$sites_polygons, fill = "black", color = "white", linewidth = 0.4, alpha = 0.75, show.legend = FALSE) +
    geom_sf(data = data$boundary_mask, fill = NA, color = "black", linewidth = 0.8, show.legend = FALSE) +
    annotation_scale(location = "bl", width_hint = 0.3) +
    annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering) +
    scale_fill_gradientn(colors = rev(terrain.colors(100)), name = NULL, guide = "none") +
    labs(title = data$name, subtitle = paste0(n_communities, " agregaciones | Threshold (P", CFG$spatial_threshold_percentile, "): ", signif(threshold, 3)), tag = plot_tags[[region_name]]) +
    coord_sf(datum = NA, expand = FALSE) +
    base_theme_map +
    theme(legend.position = "none", plot.tag = element_text(face = "bold", size = 16))
  
  # 
  idw_plot_list[[region_name]] <- p_trad
  
  # 
  total_area <- if (nrow(large_patches) > 0) sum(large_patches$count) * cell_area_ha else 0
  
  # 
  idw_summary <- data.frame(
    Region = data$name,
    IDW_Threshold = round(threshold, 3),
    Aggregations_Found = n_communities,
    Total_Aggregation_Area_ha = round(total_area, 1)
  )
  
  # 
  idw_summary_list[[region_name]] <- idw_summary
  
  # 
  cat("\n---\n\n")
}

# 
combined_idw_plot <- (idw_plot_list[["RIO_FRIO"]] | idw_plot_list[["FUQUENE"]]) / 
  (idw_plot_list[["PARITA"]] | idw_plot_list[["TONOSI"]])

# 
print(combined_idw_plot + plot_layout(guides = "keep") & theme(legend.position = "none"))

# 
cat("\n\n### Summary: IDW Results\n\n")
all_idw_summary <- do.call(rbind, idw_summary_list)
print(knitr::kable(all_idw_summary, row.names = FALSE, digits = 3))
cat("\nIDW analysis complete\n\n")
```

# 5. Method 2: HDBSCAN (Manuscript Fig. 3)
```{r fig.height=11, fig.width=8.5}
# 
hdbscan_results <- list()
hdbscan_summary_list <- list()
hdbscan_plot_list <- list()

# 
plot_tags <- c(
  RIO_FRIO = "A",
  FUQUENE  = "B",
  PARITA   = "C",
  TONOSI   = "D"
)

# 
DISTINCT_PALETTE <- c(
  "#FFB300","#803E75","#FF6800","#C10020","#007D34","#F6768E","#00538A","#53377A",
  "#FF8E00","#B32851","#F4C800","#7F180D","#93AA00","#593315","#F13A13","#232C16",
  "#E69F00","#56B4E9","#009E73","#F0E442","#0072B2","#D55E00","#CC79A7",
  "#4477AA","#66CCEE","#228833","#EE6677","#AA3377","#BBBB00","#2288BB"
)
NOISE_COLOR <- "gray60"

# 
.minPts_by_region <- list(
  DEFAULT  = 8,
  FUQUENE  = 5,
  RIO_FRIO = 8,
  PARITA   = 10,
  TONOSI   = 8
)

# 
for (region_name in names(all_data)) {
  data <- all_data[[region_name]]
  cat("Processing HDBSCAN:", data$name, "\n")
  coords <- as.matrix(data$sites_df[, c("X", "Y")])
  
  # 
  if (nrow(coords) < 3) {
    cat("  Insufficient sites for clustering.\n")
    hdbscan_results[[region_name]] <- list(
      hdb_object = NULL,
      dist_matrix = NULL,
      community_hdbscan = factor(rep(NA, nrow(coords))),
      k_used_hdbscan = NA_integer_,
      noise_count = NA_integer_
    )
    hdbscan_plot_list[[region_name]] <- ggplot() + theme_void() +
      labs(title = data$name, tag = plot_tags[[region_name]])
    next
  }
  
  # 
  has_time_cost <- exists("mobility_results") && !is.null(mobility_results[[region_name]]$D_time_h)
  if (has_time_cost) {
    D_use <- mobility_results[[region_name]]$D_time_h
    D_use <- D_use / median(D_use[D_use > 0], na.rm = TRUE)
    metric_label <- "Time-cost (DEM, scaled)"
  } else {
    D_use <- as.matrix(dist(coords)) / 1000
    metric_label <- "Euclidean (km)"
    cat("  WARNING: Using Euclidean fallback (DEM unavailable)\n")
  }
  D_dist <- as.dist(D_use)
  
  # 
  minPts <- if (!is.null(.minPts_by_region[[region_name]])) .minPts_by_region[[region_name]] else .minPts_by_region$DEFAULT
  
  # 
  set.seed(123)
  hdb_result <- dbscan::hdbscan(D_dist, minPts = minPts)
  
  # 
  labels_fac   <- factor(hdb_result$cluster)
  k_hdbscan    <- length(unique(hdb_result$cluster[hdb_result$cluster > 0]))
  noise_count  <- sum(labels_fac == "0")
  noise_pct    <- round(100 * noise_count / nrow(coords), 1)
  
  # 
  hdbscan_results[[region_name]] <- list(
    hdb_object = hdb_result,
    dist_matrix = D_use,
    community_hdbscan = labels_fac,
    k_used_hdbscan = k_hdbscan,
    noise_count = noise_count
  )
  
  # 
  cat("  k (clusters):", k_hdbscan,
      " | Noise:", noise_count, "(", noise_pct, "%)",
      " | Metric:", metric_label,
      " | minPts:", minPts, "\n")
  
  # 
  s_hdb <- data$sites_polygons
  s_hdb$comm <- labels_fac
  
  # 
  real_communities <- sort(unique(as.integer(as.character(labels_fac[labels_fac != "0"]))))
  if (length(real_communities) > length(DISTINCT_PALETTE)) {
    warning(sprintf(
      "Se requieren %d colores, pero la paleta tiene %d. Se reutilizarán colores.",
      length(real_communities), length(DISTINCT_PALETTE)
    ))
  }
  idx <- seq_len(min(length(real_communities), length(DISTINCT_PALETTE)))
  pal <- DISTINCT_PALETTE
  pal_use <- pal[idx]
  names(pal_use) <- as.character(real_communities[idx])
  if (length(real_communities) > length(DISTINCT_PALETTE)) {
    extras <- real_communities[(length(DISTINCT_PALETTE) + 1):length(real_communities)]
    pal_use <- c(pal_use, setNames(rep(DISTINCT_PALETTE, length.out = length(extras)), as.character(extras)))
  }
  color_mapping <- c("0" = NOISE_COLOR, pal_use)
  breaks_use <- if (length(real_communities) > 0) as.character(real_communities) else NULL
  labels_use <- breaks_use
  
  # 
  p_hdbscan_map <- ggplot() +
    geom_sf(data = data$boundary_mask, fill = "gray95", color = "black", linewidth = 0.8) +
    geom_sf(data = subset(s_hdb, comm != "0"), aes(fill = comm),
            color = "black", linewidth = 0.15, alpha = 0.9) +
    geom_sf(data = subset(s_hdb, comm == "0"), fill = NOISE_COLOR,
            color = "black", linewidth = 0.1, alpha = 0.7) +
    annotation_scale(location = "bl", width_hint = 0.3) +
    annotation_north_arrow(location = "tr", which_north = "true",
                           style = north_arrow_fancy_orienteering) +
    scale_fill_manual(
      values = color_mapping,
      name   = "Community",
      breaks = breaks_use,
      labels = labels_use,
      drop   = FALSE
    ) +
    guides(fill = guide_legend(override.aes = list(alpha = 1))) +
    labs(
      title = data$name,
      subtitle = paste0(k_hdbscan, " communities (k) | ", noise_count,
                        " noise sites (", noise_pct, "%) | ", metric_label,
                        " | minPts=", minPts,
                        " | palette=Distinct Global"),
      tag = plot_tags[[region_name]]
    ) +
    coord_sf(datum = NA, expand = FALSE) +
    base_theme_map +
    theme(plot.tag = element_text(face = "bold", size = 16))
  
  # 
  hdbscan_plot_list[[region_name]] <- p_hdbscan_map
  
  # 
  hdbscan_summary_list[[region_name]] <- data.frame(
    Region = data$name,
    Distance_Type = metric_label,
    HDBSCAN_minPts = minPts,
    Clusters_Found_k = k_hdbscan,
    Noise_Sites = noise_count,
    Percent_Noise = noise_pct
  )
  
  # 
  cat("\n---\n\n")
}

# 
combined_hdbscan_plot <- (hdbscan_plot_list[["RIO_FRIO"]] | hdbscan_plot_list[["FUQUENE"]]) /
  (hdbscan_plot_list[["PARITA"]]   | hdbscan_plot_list[["TONOSI"]])

# 
print(combined_hdbscan_plot + patchwork::plot_layout(guides = "collect") &
        theme(legend.position = "right"))

# 
cat("\n\n### Summary: HDBSCAN Results\n\n")
all_hdbscan_summary <- do.call(rbind, hdbscan_summary_list)
print(knitr::kable(all_hdbscan_summary, row.names = FALSE, digits = 1))
cat("\nHDBSCAN analysis complete\n\n")
```

# 6. Method 3: Spatial Networks (Manuscript Fig. 4)
```{r fig.height=11, fig.width=8.5}
network_results <- list()
network_summary_list <- list()
network_plot_list <- list()

#
plot_tags <- c(
  RIO_FRIO = "A",
  FUQUENE  = "B",
  PARITA   = "D",
  TONOSI   = "E"
)

#
.use_leiden <- "cluster_leiden" %in% getNamespaceExports("igraph")

.params_by_region <- list(
  DEFAULT  = list(resolution = 0.10, k_neighbors = 5),
  FUQUENE  = list(resolution = 0.08, k_neighbors = 6),
  RIO_FRIO = list(resolution = 0.12, k_neighbors = 5),
  PARITA   = list(resolution = 0.05, k_neighbors = 8),
  TONOSI   = list(resolution = 0.10, k_neighbors = 6)
)

for (region_name in names(all_data)) {
  data  <- all_data[[region_name]]
  coords <- as.matrix(data$sites_df[, c("X", "Y")])
  n <- nrow(coords)
  
  has_time_cost <- exists("mobility_results") && !is.null(mobility_results[[region_name]]$D_time_h)
  D_base <- if (has_time_cost) mobility_results[[region_name]]$D_time_h else as.matrix(dist(coords))
  metric_label <- if (has_time_cost) "Time-cost" else "Euclidean"
  
  if (n < 3 || is.null(D_base)) {
    network_results[[region_name]] <- list(
      graph = NULL, edges_df = NULL, stats = NULL,
      n_edges = 0, communities_obj = NULL, method = NA
    )
    cat("  Insufficient data for networks:", data$name, "\n")
    network_plot_list[[region_name]] <- ggplot() + theme_void() +
      labs(title = data$name, subtitle = "Insufficient data for network", tag = plot_tags[[region_name]])
    next
  }
  
  params <- if (!is.null(.params_by_region[[region_name]])) .params_by_region[[region_name]] else .params_by_region$DEFAULT
  k_val <- min(params$k_neighbors, n - 1)
  res_par <- params$resolution
  
  A <- matrix(0, n, n)
  for (i in 1:n) {
    ord <- order(D_base[i, ], na.last = NA)
    ord <- ord[ord != i]
    neigh <- head(ord, k_val)
    A[i, neigh] <- 1
  }
  A <- ((A + t(A)) > 0) * 1
  g <- igraph::graph_from_adjacency_matrix(A, mode = "undirected")
  
  el <- igraph::as_edgelist(g, names = FALSE)
  if (nrow(el) > 0) {
    wt <- 1 / (D_base[cbind(el[, 1], el[, 2])] + 1e-9)
    E(g)$weight <- wt
  } else {
    E(g)$weight <- numeric(0)
  }
  
  if (.use_leiden) {
    comm_obj <- tryCatch(igraph::cluster_leiden(g, weights = E(g)$weight, resolution = res_par),
                         error = function(e) list(membership = rep(1, n)))
    method_lab <- paste0("Leiden (res=", res_par, ")")
  } else {
    comm_obj <- tryCatch(igraph::cluster_louvain(g, weights = E(g)$weight),
                         error = function(e) list(membership = rep(1, n)))
    method_lab <- "Louvain"
  }
  
  Q_network <- tryCatch(
    igraph::modularity(g, membership = comm_obj$membership, weights = E(g)$weight, resolution = 1),
    error = function(e) NA_real_
  )
  k_network <- length(unique(comm_obj$membership))
  
  e <- which(A == 1, arr.ind = TRUE)
  if (nrow(e) > 0) e <- e[e[, 1] < e[, 2], , drop = FALSE]
  edges_df <- if (nrow(e) > 0) {
    data.frame(
      x1 = data$sites_df$X[e[, 1]], y1 = data$sites_df$Y[e[, 1]],
      x2 = data$sites_df$X[e[, 2]], y2 = data$sites_df$Y[e[, 2]]
    )
  } else NULL
  
  stats_df <- data.frame(
    site_id = data$sites_df$site_id,
    betweenness = igraph::betweenness(g, normalized = TRUE, weights = 1 / E(g)$weight),
    degree = igraph::degree(g),
    comm = factor(comm_obj$membership),
    X = data$sites_df$X, Y = data$sites_df$Y
  )
  
  network_results[[region_name]] <- list(
    graph = g, edges_df = edges_df, stats = stats_df,
    n_edges = ifelse(is.null(edges_df), 0, nrow(edges_df)),
    communities_obj = comm_obj, 
    method = method_lab,
    modularity_network = Q_network,
    k_network = k_network
  )
  
  cat("  (kNN-", metric_label, ", k=", k_val, ") ", method_lab,
      " | Communities (k): ", k_network, 
      " | Modularity (Q): ", round(Q_network, 3),
      " | Edges: ", ifelse(is.null(edges_df), 0, nrow(edges_df)), "\n", sep = "")
  
  real_communities <- sort(unique(comm_obj$membership))
  pal_use <- DISTINCT_PALETTE[1:min(length(real_communities), length(DISTINCT_PALETTE))]
  names(pal_use) <- as.character(real_communities[1:length(pal_use)])
  
  p_net <- ggplot() +
    geom_sf(data = data$boundary_mask, fill = "gray98", color = "black", linewidth = 0.8) +
    { if (!is.null(edges_df)) geom_segment(data = edges_df,
                                           aes(x = x1, y = y1, xend = x2, yend = y2),
                                           color = "gray60", linewidth = 0.25, alpha = 0.6) } +
    geom_point(data = stats_df, 
               aes(X, Y, size = betweenness, fill = comm), 
               shape = 21, color = "black", alpha = 0.9) +
    annotation_scale(location = "bl", width_hint = 0.3) +
    annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering) +
    scale_size_continuous(range = c(1, 5), name = "Betweenness") +
    scale_fill_manual(values = pal_use, guide = "none") +
    labs(
      title = data$name,
      subtitle = paste0("kNN (k=", k_val, ", ", metric_label, ") | ",
                        method_lab, " found k=", k_network, " communities",
                        " | Q = ", round(Q_network, 3),
                        " | Edges = ", ifelse(is.null(edges_df), 0, nrow(edges_df))),
      tag = plot_tags[[region_name]]
    ) +
    coord_sf(datum = NA, expand = FALSE) +
    base_theme_map +
    theme(
      plot.tag = element_text(face = "bold", size = 16),
      legend.position = "right",
      legend.box.margin = margin(5, 10, 5, 5),
      legend.spacing = unit(0.4, "cm")
    )
  
  network_plot_list[[region_name]] <- p_net
  
  network_summary_list[[region_name]] <- data.frame(
    Region = data$name,
    Network_Type = paste0("k-NN (", metric_label, ")"),
    Community_Method = method_lab,
    k_Neighbors = k_val,
    Communities_Found_k = k_network,
    Modularity_Network = round(Q_network, 3),
    Total_Edges = ifelse(is.null(edges_df), 0, nrow(edges_df)),
    Mean_Betweenness = round(mean(stats_df$betweenness, na.rm = TRUE), 4)
  )
  
  cat("\n---\n")
}

#
combined_network_plot <- (network_plot_list[["RIO_FRIO"]] | network_plot_list[["FUQUENE"]]) /
  (network_plot_list[["PARITA"]] | network_plot_list[["TONOSI"]])

#
print(combined_network_plot & theme(legend.position = "right"))

cat("\n\n### Table — Network Metrics Summary {#tab-network-metrics}\n\n")
all_network_summary <- do.call(rbind, network_summary_list)

print(
  tbl_kable(
    all_network_summary,
    caption = "Table. Network Metrics Summary",
    label   = "network-metrics",
    row.names = FALSE, digits = 4
  )
)

#
cat("\nNetwork (kNN) analysis complete\n\n")
```

# 7. Critical Spatial Connectivity (Manuscript Fig. 5)
```{r}
# 
all_curves <- list()
crit_table <- list()

# 
for (region_name in names(all_data)) {
  data   <- all_data[[region_name]]
  coords <- as.matrix(data$sites_df[, c("X", "Y")])
  
  crit_m <- NA_real_
  df_m   <- data.frame(threshold_m = numeric(0), prop_in_largest = numeric(0))
  
  if (nrow(coords) >= 2) {
    Dm <- as.matrix(dist(coords))
    thresholds <- seq(100, 5000, by = 100)
    largest <- vapply(thresholds, function(th) {
      adj <- Dm <= th; diag(adj) <- FALSE
      g <- igraph::graph_from_adjacency_matrix(adj, mode = "undirected")
      max(igraph::components(g)$csize)
    }, numeric(1))
    prop <- largest / nrow(coords)
    
    crit_idx <- which(prop >= 0.5)[1]
    if (!is.na(crit_idx)) crit_m <- thresholds[crit_idx]
    
    df_m <- data.frame(
      region = data$name,
      threshold_m = thresholds,
      prop_in_largest = prop
    )
    all_curves[[region_name]] <- df_m
    
    crit_table[[region_name]] <- data.frame(
      region = data$name,
      crit_m = crit_m,
      crit_prop = if (!is.na(crit_m)) prop[crit_idx] else NA_real_
    )
  }
}

# 
if (length(all_curves) > 0) {
  all_curves_df <- do.call(rbind, all_curves)
  crit_df <- do.call(rbind, crit_table)
  crit_df <- subset(crit_df, !is.na(crit_m) & !is.na(crit_prop))
  
  # 
  dist_data <- dplyr::bind_rows(lapply(names(all_data), function(rn) {
    coords <- as.matrix(all_data[[rn]]$sites_df[, c("X", "Y")])
    if (nrow(coords) < 2) return(NULL)
    D <- as.matrix(dist(coords))
    D_vec <- D[upper.tri(D)]
    data.frame(
      region   = all_data[[rn]]$name,
      distance = D_vec
    )
  }))
  
  # 
  dist_stats <- dist_data %>%
    dplyr::group_by(region) %>%
    dplyr::summarise(
      mean_dist   = mean(distance, na.rm = TRUE),
      median_dist = median(distance, na.rm = TRUE),
      sd_dist     = sd(distance, na.rm = TRUE),
      .groups     = "drop"
    ) %>%
    dplyr::left_join(crit_df, by = "region")
  
  # 
  binwidth_m <- 250
  x_limits   <- c(0, 5000)
  x_breaks   <- seq(0, 5000, 1000)
  breaks_vec <- seq(x_limits[1], x_limits[2], by = binwidth_m)
  
  # 
  dist_data <- dist_data %>%
    dplyr::left_join(crit_df[, c("region", "crit_m")], by = "region")
  
  # 
  dist_binned <- dist_data %>%
    dplyr::mutate(
      bin = cut(distance, breaks = breaks_vec, right = FALSE, include.lowest = TRUE),
      bin_start  = as.numeric(sub("\\[(.*),.*", "\\1", bin)),
      bin_end    = bin_start + binwidth_m,
      bin_center = bin_start + binwidth_m / 2
    ) %>%
    dplyr::group_by(region, crit_m, bin_start, bin_end, bin_center) %>%
    dplyr::summarise(count = dplyr::n(), .groups = "drop")
  
  # 
  all_bins <- expand.grid(
    region    = unique(dist_data$region),
    bin_start = breaks_vec[-length(breaks_vec)],
    stringsAsFactors = FALSE
  )
  all_bins$bin_end    <- all_bins$bin_start + binwidth_m
  all_bins$bin_center <- all_bins$bin_start + binwidth_m / 2
  
  # 
  dist_binned <- all_bins %>%
    dplyr::left_join(dist_binned, by = c("region","bin_start","bin_end","bin_center")) %>%
    dplyr::left_join(crit_df[, c("region", "crit_m")], by = "region") %>%
    dplyr::mutate(count = dplyr::coalesce(count, 0L)) %>%
    dplyr::arrange(region, bin_start)
  
  # 
  dist_binned <- dist_binned %>%
    dplyr::mutate(signed_from_crit = bin_center - crit_m)
  
  # 
  plasma_rev <- viridisLite::plasma(256, direction = -1)
  
  # 
  p_hist <- ggplot(dist_binned, aes(x = bin_center, y = count)) +
    geom_col(aes(fill = signed_from_crit), width = binwidth_m, color = "black", linewidth = 0.2) +
    geom_vline(
      data = dist_stats, aes(xintercept = crit_m),
      color = "black", linetype = "dashed", linewidth = 1
    ) +
    geom_text(
      data = dist_stats,
      aes(x = Inf, y = Inf,
          label = paste0(
            "Median: ", round(median_dist, 0), " m\n",
            "Critical: ", round(crit_m, 0), " m"
          )),
      hjust = 1.1, vjust = 1.5, size = 3.5, fontface = "bold", color = "black",
      inherit.aes = FALSE
    ) +
    facet_wrap(~ region, ncol = 2, scales = "free_y") +
    scale_x_continuous(
      limits = x_limits, breaks = x_breaks, expand = c(0, 0),
      labels = scales::comma
    ) +
    scale_y_continuous(expand = c(0, 0)) +
    scale_fill_gradientn(
      colours = plasma_rev,
      name = "Distance\nrelative to\ncritical (m)",
      limits = c(min(dist_binned$signed_from_crit, na.rm = TRUE),
                 max(dist_binned$signed_from_crit, na.rm = TRUE)),
      guide = guide_colorbar(barheight = unit(50, "pt"))
    ) +
    labs(
      title = "Inter-site Distance Distribution",
      subtitle = "Dashed black: critical threshold | Fill: plasma (reversed) ~ distance vs. threshold",
      x = "Distance (m)",
      y = "Frequencies"
    ) +
    theme_bw(base_size = 12) +
    theme(
      plot.title       = element_text(face = "bold", size = 14, hjust = 0.5),
      plot.subtitle    = element_text(size = 10,  hjust = 0.5, margin = margin(b = 10)),
      strip.background = element_rect(fill = "gray90", color = "black"),
      strip.text       = element_text(face = "bold", size = 11),
      panel.grid.minor = element_blank(),
      panel.spacing    = unit(6, "pt"),
      legend.position  = "right"
    )
  
  # 
  print(p_hist)
  
  # 
  cat("\n\n### Table — Critical Connectivity Thresholds {#tab-critical-connectivity}\n\n")
  table_data <- dist_stats[, c("region", "median_dist", "crit_m", "crit_prop")]
  names(table_data) <- c("Region", "Median Distance (m)", "Critical Threshold (m)", "Proportion at Threshold")
  
  print(
    tbl_kable(
      table_data,
      caption = "Table. Critical Connectivity Thresholds",
      label   = "critical-connectivity",
      digits = 1, row.names = FALSE
    )
  )
}

# 
cat("\n✓ Connectivity analysis complete\n\n")
```

# 8. Method Convergence (ARI) (Manuscript Fig. 6)
```{r fig.height=11, fig.width=8.5}
# 
community_detect_results <- list()
.has_leiden <- "cluster_leiden" %in% getNamespaceExports("igraph")
net_comm_table <- data.frame()

# 
.params_by_region <- list(
  DEFAULT  = list(resolution = 0.10),
  FUQUENE  = list(resolution = 0.08),
  RIO_FRIO = list(resolution = 0.12),
  PARITA   = list(resolution = 0.05),
  TONOSI   = list(resolution = 0.10)
)

# 
for (region_name in names(all_data)) {
  data <- all_data[[region_name]]
  n <- nrow(data$sites_df)
  
  if (n < 3) {
    cat(">", data$name, ": insufficient sites for network detection\n")
    next
  }
  
  G <- network_results[[region_name]]$graph
  if (is.null(G)) {
    cat(">", data$name, ": network not available\n")
    next
  }
  
  # 
  params <- if (!is.null(.params_by_region[[region_name]])) .params_by_region[[region_name]] else .params_by_region$DEFAULT
  res_par <- params$resolution
  
  # 
  leid <- tryCatch(
    if (.has_leiden) igraph::cluster_leiden(G, weights = E(G)$weight, resolution = res_par)$membership else rep(NA_integer_, n),
    error = function(e) rep(NA_integer_, n)
  )
  
  # 
  louv <- tryCatch(
    igraph::cluster_louvain(G, weights = E(G)$weight)$membership,
    error = function(e) rep(NA_integer_, n)
  )
  
  # 
  idw_assign <- traditional_results[[region_name]]$site_assignments
  hdb_assign <- as.numeric(hdbscan_results[[region_name]]$community_hdbscan)
  
  # 
  ari_leid_hdb <- ari_safe(leid, hdb_assign)
  ari_leid_idw <- ari_safe(leid, idw_assign)
  ari_idw_hdb  <- ari_safe(idw_assign, hdb_assign)
  ari_louv_hdb <- ari_safe(louv, hdb_assign)
  ari_louv_idw <- ari_safe(louv, idw_assign)
  
  # 
  community_detect_results[[region_name]] <- list(
    membership_leiden  = leid,
    membership_louvain = louv,
    ari_leiden_hdb     = ari_leid_hdb,
    ari_leiden_idw     = ari_leid_idw,
    ari_idw_hdb        = ari_idw_hdb,
    resolution_used    = res_par
  )
  
  # 
  net_comm_table <- rbind(net_comm_table, data.frame(
    region               = data$name,
    n_sites              = n,
    leiden_k             = if (.has_leiden) length(unique(na.omit(leid))) else NA_integer_,
    leiden_resolution    = res_par,
    # CORRECCIÓN: Usar el valor pre-calculado almacenado en la lista de resultados
    hdbscan_k            = hdbscan_results[[region_name]]$k_used_hdbscan, 
    idw_k                = length(unique(na.omit(idw_assign[idw_assign > 0]))),
    ARI_Leiden_HDBSCAN   = if (.has_leiden) ari_leid_hdb else NA_real_,
    ARI_Leiden_IDW       = if (.has_leiden) ari_leid_idw else NA_real_,
    ARI_IDW_HDBSCAN      = ari_idw_hdb
  ))
}

# 
if (nrow(net_comm_table) > 0) {
  region_order <- net_comm_table %>% arrange(ARI_Leiden_HDBSCAN) %>% pull(region)
  net_comm_table$region <- factor(net_comm_table$region, levels = region_order)
  
  # 
  k_long <- net_comm_table %>%
    dplyr::select(region, leiden_k, hdbscan_k, idw_k) %>%
    tidyr::pivot_longer(-region, names_to = "method", values_to = "k") %>%
    dplyr::mutate(
      method_label = dplyr::recode(method,
                                   leiden_k  = "Leiden (Network)",
                                   hdbscan_k = "HDBSCAN",
                                   idw_k     = "IDW"),
      method_label = factor(method_label, levels = c("Leiden (Network)", "HDBSCAN", "IDW"))
    )
  
  # 
  p_k <- ggplot(k_long, aes(x = k, y = region, color = method_label, shape = method_label)) +
    geom_point(size = 4, alpha = 0.9) +
    geom_line(aes(group = region), color = "gray60", linewidth = 0.5, alpha = 0.5) +
    scale_color_manual(values = c("Leiden (Network)" = "#1b9e77", "HDBSCAN" = "#d95f02", "IDW" = "#7570b3"), name = "Method") +
    scale_shape_manual(values = c("Leiden (Network)" = 16, "HDBSCAN" = 17, "IDW" = 15), name = "Method") +
    scale_x_continuous(breaks = seq(0, 20, 5)) +
    labs(title = "Number of Communities (k) by Method",
         subtitle = "Comparison of clustering granularity across methods",
         x = "Number of Communities (k)", y = NULL, tag = "A") +
    theme_bw(base_size = 11) +
    theme(plot.title = element_text(face = "bold", size = 13),
          plot.subtitle = element_text(size = 10),
          plot.tag = element_text(face = "bold", size = 18),
          legend.position = "right")
  
  # 
  ari_long <- net_comm_table %>%
    dplyr::select(region, ARI_Leiden_HDBSCAN, ARI_Leiden_IDW, ARI_IDW_HDBSCAN) %>%
    tidyr::pivot_longer(-region, names_to = "comparison", values_to = "ARI") %>%
    dplyr::mutate(
      comp_label = dplyr::recode(comparison,
                                 ARI_Leiden_HDBSCAN = "Leiden ~ HDBSCAN",
                                 ARI_Leiden_IDW     = "Leiden ~ IDW",
                                 ARI_IDW_HDBSCAN    = "IDW ~ HDBSCAN"),
      comp_label = factor(comp_label, levels = c("Leiden ~ HDBSCAN", "Leiden ~ IDW", "IDW ~ HDBSCAN"))
    )
  
  # 
  p_ari <- ggplot(ari_long, aes(x = ARI, y = region, shape = comp_label, color = comp_label)) +
    geom_vline(xintercept = 0.70, linetype = "dashed", color = "green4", linewidth = 0.6, alpha = 0.7) +
    geom_vline(xintercept = 0.30, linetype = "dashed", color = "red3", linewidth = 0.6, alpha = 0.7) +
    annotate("text", x = 0.70, y = Inf, label = "Strong (0.70)", vjust = -0.5, hjust = 1.1, size = 3, color = "green4", fontface = "bold") +
    annotate("text", x = 0.30, y = Inf, label = "Weak (0.30)", vjust = -0.5, hjust = -0.1, size = 3, color = "red3", fontface = "bold") +
    geom_point(size = 3.5, alpha = 0.9) +
    coord_cartesian(xlim = c(0, 1)) +
    scale_color_viridis_d(name = "Comparison", option = "D") +
    scale_shape_manual(values = c("Leiden ~ HDBSCAN" = 16, "Leiden ~ IDW" = 17, "IDW ~ HDBSCAN" = 15), name = "Comparison") +
    labs(title = "Method Convergence (Adjusted Rand Index)",
         subtitle = "Agreement between clustering methods (0 = random, 1 = perfect)",
         x = "ARI (0–1)", y = NULL, tag = "B") +
    theme_bw(base_size = 11) +
    theme(plot.title = element_text(face = "bold", size = 13),
          plot.subtitle = element_text(size = 10),
          plot.tag = element_text(face = "bold", size = 18),
          legend.position = "right")
  
  # 
  combined_ari_panel <- (p_k / p_ari) + patchwork::plot_layout(guides = "collect") +
    patchwork::plot_annotation(title = "Clustering Method Comparison",
                               theme = theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5)))
  
  # 
  print(combined_ari_panel & theme(legend.position = "right"))
  
  # 
  cat("\n\n### Table — Method Convergence (ARI) and Network Detection {#tab-ari-convergence}\n\n")
  net_comm_table_print <- net_comm_table %>%
    dplyr::mutate(across(where(is.numeric), ~ round(.x, 3))) %>%
    dplyr::select(region, n_sites, leiden_k, leiden_resolution, hdbscan_k, idw_k,
                  ARI_Leiden_HDBSCAN, ARI_Leiden_IDW, ARI_IDW_HDBSCAN)
  
  # 
  if (!exists("tbl_kable")) {
    tbl_kable <- function(df, caption = NULL, label = NULL, ...) {
      knitr::kable(df, caption = caption, ...)
    }
  }
  
  # 
  print(
    tbl_kable(
      net_comm_table_print,
      caption = "Table. Method Convergence (Adjusted Rand Index) and Network Detection Summary",
      label   = "ari-convergence",
      format = "simple"
    )
  )
  cat("\n")
}

# 
cat("\n✓ ARI convergence analysis complete\n\n")
```

# 9. Pre-calculation of Summary Statistics for Plots
```{r}
# 
all_stats_temp <- data.frame()
nnd_data_list_temp <- list()

# 
for (region_name in names(all_data)) {
  data <- all_data[[region_name]]
  trad <- traditional_results[[region_name]]
  hdb  <- hdbscan_results[[region_name]]
  net  <- network_results[[region_name]]
  com  <- community_detect_results[[region_name]]

  pts <- st_as_sf(data$sites_df, coords = c("X", "Y"), crs = data$crs)
  D_nnd <- as.matrix(dist(st_coordinates(pts))); diag(D_nnd) <- NA
  nnd_obs_vec <- apply(D_nnd, 1, min, na.rm = TRUE)
  mean_nnd_obs <- mean(nnd_obs_vec, na.rm = TRUE)
  nnd_data_list_temp[[region_name]] <- data.frame(region = data$name, nnd_m = nnd_obs_vec)

  crit_df <- tryCatch(do.call(rbind, crit_table), error = function(e) NULL)
  conn_thresh_m <- if (!is.null(crit_df) && any(crit_df$region == data$name)) {
    crit_df$crit_m[crit_df$region == data$name][1]
  } else NA_real_

  net_mod <- if (!is.null(net$modularity_network) && length(net$modularity_network) == 1) {
    net$modularity_network
  } else NA_real_

  stats_temp <- data.frame(
    region = data$name,
    n_sites = nrow(data$sites_df),
    total_area_ha = sum(data$sites_df$area_ha),
    mean_site_area_ha = mean(data$sites_df$area_ha),
    mean_nnd_m = mean_nnd_obs,
    idw_communities = trad$n_communities,
    hdbscan_communities = hdb$k_used_hdbscan,
    hdbscan_noise_pct = 100 * hdb$noise_count / nrow(data$sites_df),
    network_modularity = net_mod,
    network_edges = net$n_edges,
    ari_idw_hdb = com$ari_idw_hdb,
    connectivity_threshold_m = conn_thresh_m
  )

  all_stats_temp <- rbind(all_stats_temp, stats_temp)
}

# 
all_stats <- all_stats_temp

# 
sites_by_region <- dplyr::bind_rows(lapply(names(all_data), function(rn) {
  df <- all_data[[rn]]$sites_df
  data.frame(region = all_data[[rn]]$name, area_ha = df$area_ha)
}))

# 
nnd_by_region <- do.call(rbind, nnd_data_list_temp)

# 
rm(all_stats_temp, nnd_data_list_temp)

# 
cat("✓ Summary statistics pre-calculated for subsequent sections.\n\n")
```

# 10. Bullet Graphs (Manuscript Fig. 7)
```{r fig.height=11, fig.width=8.5}
# 
if (!exists("SER")) {
  SER <- function(x) {
    x <- x[is.finite(x)]
    if (length(x) <= 1) return(NA_real_)
    stats::sd(x, na.rm = TRUE) / sqrt(length(x))
  }
}

# 
if (!exists("area_summary")) {
  if (!exists("sites_by_region")) stop("Falta 'sites_by_region' para construir area_summary.")
  area_summary <- sites_by_region %>%
    dplyr::group_by(region) %>%
    dplyr::summarise(
      N = dplyr::n(),
      media = mean(area_ha, na.rm = TRUE),
      ser = SER(area_ha),
      .groups = "drop"
    ) %>%
    dplyr::mutate(
      p80 = qt(0.90, pmax(N - 1, 1)),
      p95 = qt(0.975, pmax(N - 1, 1)),
      p99 = qt(0.995, pmax(N - 1, 1))
    )
}

# 
if (!exists("nnd_summary")) {
  if (!exists("nnd_by_region")) stop("Falta 'nnd_by_region' para construir nnd_summary.")
  nnd_summary <- nnd_by_region %>%
    dplyr::group_by(region) %>%
    dplyr::summarise(
      N = dplyr::n(),
      media = mean(nnd_m, na.rm = TRUE),
      ser = SER(nnd_m),
      .groups = "drop"
    ) %>%
    dplyr::mutate(
      p80 = qt(0.90, pmax(N - 1, 1)),
      p95 = qt(0.975, pmax(N - 1, 1)),
      p99 = qt(0.995, pmax(N - 1, 1))
    )
}

# 
if (!exists("mod_summary")) {
  if (!exists("all_stats")) stop("Falta 'all_stats' para construir mod_summary.")
  mod_summary <- all_stats %>%
    dplyr::select(region, network_modularity) %>%
    dplyr::mutate(
      N = 1L,
      media = network_modularity,
      ser = 0.05 * abs(network_modularity),
      p80 = 1.282, p95 = 1.960, p99 = 2.576
    ) %>% dplyr::select(region, N, media, ser, p80, p95, p99)
}

# 
if (!exists("conn_summary")) {
  if (!exists("all_stats")) stop("Falta 'all_stats' para construir conn_summary.")
  conn_summary <- all_stats %>%
    dplyr::select(region, connectivity_threshold_m) %>%
    dplyr::mutate(
      N = 1L,
      media = connectivity_threshold_m,
      ser = 0.10 * abs(connectivity_threshold_m),
      p80 = 1.282, p95 = 1.960, p99 = 2.576
    ) %>% dplyr::select(region, N, media, ser, p80, p95, p99)
}

# 
.oldpar <- par(no.readonly = TRUE)
on.exit(par(.oldpar), add = TRUE)
par(mfrow = c(1, 4), mar = c(6, 5, 4.5, 1) + 0.1)
par(lend = 1)

# 
mid_len <- 0.40
mid_lwd <- 2

# 
if (nrow(area_summary) > 0) {
  xA <- seq_len(nrow(area_summary)); yA <- area_summary$media
  ymin <- max(0, min(yA - 1.1 * area_summary$p99 * area_summary$ser, na.rm = TRUE))
  ymax <- max(yA + 1.1 * area_summary$p99 * area_summary$ser, na.rm = TRUE)
  plot(xA, yA, pch = NA, xlab = "Region", ylab = "Mean Site Area (ha)", xaxt = "n",
       xlim = c(0.5, nrow(area_summary) + 0.5), ylim = c(ymin, ymax),
       main = "Mean Site Area by Region", sub = "80/95/99% CIs shown as bars")
  arrows(xA, yA - area_summary$ser * area_summary$p99, xA, yA + area_summary$ser * area_summary$p99,
         length = 0, angle = 90, code = 3, lwd = 1.2, col = 1)
  arrows(xA, yA - area_summary$ser * area_summary$p95, xA, yA + area_summary$ser * area_summary$p95,
         length = 0, angle = 90, code = 3, lwd = 5, col = 1)
  arrows(xA, yA - area_summary$ser * area_summary$p80, xA, yA + area_summary$ser * area_summary$p80,
         length = 0, angle = 90, code = 3, lwd = 10, col = 1)
  segments(xA - mid_len, yA, xA + mid_len, yA, lwd = mid_lwd)
  axis(1, at = xA, labels = area_summary$region, las = 1, cex.axis = 0.9)
  mtext("A", side = 3, adj = 0, line = 0.5, font = 2, cex = 1.4)
} else { plot.new(); mtext("A — Mean Site Area (no data)", 3, line = -2, cex = 1.0, font = 2) }

# 
if (nrow(nnd_summary) > 0) {
  xB <- seq_len(nrow(nnd_summary)); yB <- nnd_summary$media
  ymin <- max(0, min(yB - 1.1 * nnd_summary$p99 * nnd_summary$ser, na.rm = TRUE))
  ymax <- max(yB + 1.1 * nnd_summary$p99 * nnd_summary$ser, na.rm = TRUE)
  plot(xB, yB, pch = NA, xlab = "Region", ylab = "Mean NND (m)", xaxt = "n",
       xlim = c(0.5, nrow(nnd_summary) + 0.5), ylim = c(ymin, ymax),
       main = "Mean Nearest Neighbor Distance (NND)", sub = "80/95/99% CIs shown as bars")
  arrows(xB, yB - nnd_summary$ser * nnd_summary$p99, xB, yB + nnd_summary$ser * nnd_summary$p99,
         length = 0, angle = 90, code = 3, lwd = 1.2, col = 1)
  arrows(xB, yB - nnd_summary$ser * nnd_summary$p95, xB, yB + nnd_summary$ser * nnd_summary$p95,
         length = 0, angle = 90, code = 3, lwd = 5, col = 1)
  arrows(xB, yB - nnd_summary$ser * nnd_summary$p80, xB, yB + nnd_summary$ser * nnd_summary$p80,
         length = 0, angle = 90, code = 3, lwd = 10, col = 1)
  segments(xB - mid_len, yB, xB + mid_len, yB, lwd = mid_lwd)
  axis(1, at = xB, labels = nnd_summary$region, las = 1, cex.axis = 0.9)
  mtext("B", side = 3, adj = 0, line = 0.5, font = 2, cex = 1.4)
} else { plot.new(); mtext("B — Mean NND (no data)", 3, line = -2, cex = 1.0, font = 2) }

# 
if (nrow(mod_summary) > 0) {
  xC <- seq_len(nrow(mod_summary)); yC <- mod_summary$media
  ymax <- max(0.4, max(yC + 1.1 * mod_summary$p99 * mod_summary$ser, na.rm = TRUE))
  plot(xC, yC, pch = NA, xlab = "Region", ylab = "Modularity (Q)", xaxt = "n",
       xlim = c(0.5, nrow(mod_summary) + 0.5), ylim = c(0, ymax),
       main = "Network Modularity (kNN vs HDBSCAN)", sub = "Dashed line at Q ≈ 0.30")
  arrows(xC, pmax(0, yC - mod_summary$ser * mod_summary$p99), xC, yC + mod_summary$ser * mod_summary$p99,
         length = 0, angle = 90, code = 3, lwd = 1.2, col = 1)
  arrows(xC, pmax(0, yC - mod_summary$ser * mod_summary$p95), xC, yC + mod_summary$ser * mod_summary$p95,
         length = 0, angle = 90, code = 3, lwd = 5, col = 1)
  arrows(xC, pmax(0, yC - mod_summary$ser * mod_summary$p80), xC, yC + mod_summary$ser * mod_summary$p80,
         length = 0, angle = 90, code = 3, lwd = 10, col = 1)
  segments(xC - mid_len, yC, xC + mid_len, yC, lwd = mid_lwd)
  axis(1, at = xC, labels = mod_summary$region, las = 1, cex.axis = 0.9)
  abline(h = 0.3, lty = 2, col = "red")
  mtext("C", side = 3, adj = 0, line = 0.5, font = 2, cex = 1.4)
} else { plot.new(); mtext("C — Network Modularity (no data)", 3, line = -2, cex = 1.0, font = 2) }

# 
if (nrow(conn_summary) > 0) {
  xD <- seq_len(nrow(conn_summary)); yD <- conn_summary$media
  ymax <- max(yD + 1.1 * conn_summary$p99 * conn_summary$ser, na.rm = TRUE)
  plot(xD, yD, pch = NA, xlab = "Region", ylab = "Critical Threshold (m)", xaxt = "n",
       xlim = c(0.5, nrow(conn_summary) + 0.5), ylim = c(0, ymax),
       main = "Critical Connectivity Threshold (50%)", sub = "80/95/99% CIs shown as bars")
  arrows(xD, pmax(0, yD - conn_summary$ser * conn_summary$p99), xD, yD + conn_summary$ser * conn_summary$p99,
         length = 0, angle = 90, code = 3, lwd = 1.2, col = 1)
  arrows(xD, pmax(0, yD - conn_summary$ser * conn_summary$p95), xD, yD + conn_summary$ser * conn_summary$p95,
         length = 0, angle = 90, code = 3, lwd = 5, col = 1)
  arrows(xD, pmax(0, yD - conn_summary$ser * conn_summary$p80), xD, yD + conn_summary$ser * conn_summary$p80,
         length = 0, angle = 90, code = 3, lwd = 10, col = 1)
  segments(xD - mid_len, yD, xD + mid_len, yD, lwd = mid_lwd)
  axis(1, at = xD, labels = conn_summary$region, las = 1, cex.axis = 0.9)
  mtext("D", side = 3, adj = 0, line = 0.5, font = 2, cex = 1.4)
} else { plot.new(); mtext("D — Connectivity Threshold (no data)", 3, line = -2, cex = 1.0, font = 2) }

# 
cat("\n✓ Synthesis bullet graphs complete (combined 4-panel figure)\n\n")

# 
region_levels <- if (exists("area_summary") && nrow(area_summary) > 0) {
  as.character(area_summary$region)
} else if (exists("nnd_summary") && nrow(nnd_summary) > 0) {
  as.character(nnd_summary$region)
} else if (exists("mod_summary") && nrow(mod_summary) > 0) {
  as.character(mod_summary$region)
} else if (exists("conn_summary") && nrow(conn_summary) > 0) {
  as.character(conn_summary$region)
} else character(0)

# 
tabla_final_balas <- dplyr::bind_rows(
  dplyr::transmute(area_summary, metric = "A. Mean Site Area (ha)", region, N, mean = media, SER = ser, p80, p95, p99),
  dplyr::transmute(nnd_summary,  metric = "B. Mean NND (m)",        region, N, mean = media, SER = ser, p80, p95, p99),
  dplyr::transmute(mod_summary,  metric = "C. Network Modularity (Q)", region, N, mean = media, SER = ser, p80, p95, p99),
  dplyr::transmute(conn_summary, metric = "D. Connectivity Threshold (m)", region, N, mean = media, SER = ser, p80, p95, p99)
) %>%
  dplyr::mutate(region = factor(region, levels = region_levels)) %>%
  dplyr::arrange(metric, region)

# 
cat("\n### Summary Table — Bullet Graphs (A–D)\n\n")
print(knitr::kable(
  tabla_final_balas,
  digits = c(NA, NA, 0, 3, 3, 3, 3, 3),
  align  = c("l","l","r","r","r","r","r","r")
))
```

# 10.1. Detailed nearest neighbor analysis
```{r}
# 
.get_region_area_m2 <- function(boundary_sf, pts_sf, crs_obj) {
  b <- boundary_sf %>% st_make_valid() %>% st_transform(crs_obj)
  gtypes <- unique(as.character(st_geometry_type(b)))
  poly <- NULL
  if (all(gtypes %in% c("POLYGON","MULTIPOLYGON"))) {
    poly <- st_union(b)
  } else if (all(gtypes %in% c("LINESTRING","MULTILINESTRING"))) {
    poly_sfc <- st_polygonize(st_geometry(b))
    poly_sf  <- st_as_sf(st_collection_extract(poly_sfc, "POLYGON"))
    if (nrow(poly_sf) == 0) {
      poly <- st_convex_hull(st_union(pts_sf)) |> st_as_sf()
    } else {
      counts <- lengths(st_intersects(poly_sf, pts_sf))
      idx <- if (any(counts > 0)) which.max(counts) else which.max(st_area(poly_sf))
      poly <- poly_sf[idx,]
    }
  } else {
    poly <- st_convex_hull(st_union(pts_sf)) |> st_as_sf()
  }
  as.numeric(st_area(st_union(poly)))
}

# 
nearest_neighbor_by_region <- dplyr::bind_rows(lapply(names(all_data), function(region_name) {
  dat <- all_data[[region_name]]
  n   <- nrow(dat$sites_df)
  if (n < 2) {
    return(data.frame(
      Region = dat$name, N_sites = n,
      R_index = NA_real_, Z_value = NA_real_, p_value = NA_real_,
      Diff_ENND_MNND_m = NA_real_, CI95_low_m = NA_real_, CI95_high_m = NA_real_
    ))
  }
  pts_sf <- st_as_sf(dat$sites_df, coords = c("X","Y"), crs = dat$crs)
  A_m2 <- .get_region_area_m2(dat$boundary_mask, pts_sf, dat$crs)
  xy  <- st_coordinates(pts_sf)
  D   <- as.matrix(dist(xy)); diag(D) <- NA
  nnd <- apply(D, 1, min, na.rm = TRUE)
  MNND <- mean(nnd)
  ENND <- 0.5 / sqrt(n / A_m2)
  Rval <- MNND / ENND
  SE <- (0.26136 * sqrt(A_m2)) / n
  Z  <- (ENND - MNND) / SE
  p2 <- 2 * pnorm(abs(Z), lower.tail = FALSE)
  diff <- ENND - MNND
  ci_l <- diff - 1.96 * SE
  ci_h <- diff + 1.96 * SE
  data.frame(
    Region = dat$name,
    N_sites = n,
    R_index = Rval,
    Z_value = Z,
    p_value = p2,
    Diff_ENND_MNND_m = diff,
    CI95_low_m = ci_l,
    CI95_high_m = ci_h
  )
}))

# 
nearest_neighbor_by_region_print <- nearest_neighbor_by_region %>%
  dplyr::mutate(
    R_index = round(R_index, 3),
    Z_value = round(Z_value, 3),
    p_value = signif(p_value, 3),
    Diff_ENND_MNND_m = round(Diff_ENND_MNND_m, 2),
    CI95_low_m = round(CI95_low_m, 2),
    CI95_high_m = round(CI95_high_m, 2)
  )

# 
print(knitr::kable(
  nearest_neighbor_by_region_print,
  align = "lccccccc",
  col.names = c("Region","N","R","Z","p","ENND−MNND (m)","CI 95% low (m)","CI 95% high (m)")
))
```

# 10.2. Point Pattern Analysis (K/L Function) - Reviewer 2 Request
```{r fig.height=12, fig.width=8.5}
k_func_results <- list()
k_plot_list <- list()

if (requireNamespace("spatstat.geom", quietly = TRUE) && 
    requireNamespace("spatstat.explore", quietly = TRUE)) {
  
  for (region_name in names(all_data)) {
    data <- all_data[[region_name]]
    cat("Calculating Ripley's K (L-transformed) for:", data$name, "...\n")
    
    # 1. Setup Point Pattern
    coords <- st_coordinates(data$sites_centroids)
    
    # Use Convex Hull for the window to define the study area shape accurately
    window_owin <- tryCatch({
      as.owin(sf::st_convex_hull(sf::st_union(data$sites_centroids)))
    }, error = function(e) {
      as.owin(sf::st_bbox(data$sites_centroids))
    })
    
    ppp_obj <- spatstat.geom::ppp(x = coords[,1], y = coords[,2], window = window_owin, check = FALSE)
    
    # 2. Calculate Envelopes using 'Lest' directly 
    # (Lest is just a transformation of Kest, statistically identical but better for plotting)
    k_env <- tryCatch({
      spatstat.explore::envelope(
        ppp_obj, 
        fun = spatstat.explore::Lest, 
        nsim = 1000, 
        correction = "best", 
        global = TRUE, 
        verbose = FALSE
      )
    }, error = function(e) NULL)
    
    if (is.null(k_env)) next
    
    # 3. Transform Data: Centering on Zero (L(r) - r)
    # This turns the theoretical line into a horizontal line at y=0
    df_k <- as.data.frame(k_env)
    df_plot <- data.frame(
      r = df_k$r,
      obs = df_k$obs - df_k$r,  # Centering Observed
      theo = 0,                 # Centering Theoretical (becomes 0)
      hi = df_k$hi - df_k$r,    # Centering High Envelope
      lo = df_k$lo - df_k$r,    # Centering Low Envelope
      region = data$name
    )
    
    # Crop distance to relevant scale (e.g., 40% of window) to zoom in on community patterns
    max_r_plot <- max(df_plot$r) * 0.4
    df_plot <- df_plot[df_plot$r <= max_r_plot, ]
    
    k_func_results[[region_name]] <- df_plot
    
    # 4. Create the Informative Plot (L(r)-r)
    p_k <- ggplot(df_plot, aes(x = r)) +
      # Envelopes (Gray Band)
      geom_ribbon(aes(ymin = lo, ymax = hi), fill = "gray70", alpha = 0.4) +
      # Theoretical Line (0)
      geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 0.6) +
      # Observed Line
      geom_line(aes(y = obs), color = "black", linewidth = 0.8) +
      # Labels indicating meaning
      annotate("text", x = max(df_plot$r), y = max(df_plot$hi), label = "Clustering", 
               vjust = 1, hjust = 1, size = 3, color = "gray40", fontface="italic") +
      labs(
        title = data$name,
        subtitle = "Centered L-Function (L(r) - r)",
        y = "Deviation from CSR",
        x = "Distance (m)", 
        tag = plot_tags[[region_name]]
      ) +
      theme_bw(base_size = 11) +
      theme(
        plot.title = element_text(face = "bold", size = 12),
        plot.subtitle = element_text(size = 9),
        plot.tag = element_text(face = "bold", size = 14)
      )
    
    k_plot_list[[region_name]] <- p_k
  }
}

# 5. Combined Plot
if (length(k_plot_list) >= 4) {
  # Apply layout without breaking ggplot objects
  combined_plot <- (k_plot_list[["RIO_FRIO"]] | k_plot_list[["FUQUENE"]]) / 
    (k_plot_list[["PARITA"]]    | k_plot_list[["TONOSI"]])
  
  # Add annotation separately
  final_plot <- combined_plot + 
    patchwork::plot_annotation(
      title = "Figure 9. Multi-scalar Spatial Structure (L-function)",
      subtitle = "Values > Gray Band: Statistically Significant Clustering.\nValues < Gray Band: Significant Dispersion.\nGray Band: 99% Confidence Envelope for Randomness (CSR).",
      theme = theme(plot.title = element_text(face="bold", size=14))
    )
  
  print(final_plot)
}

# 6. Improved Summary Table
summarize_l_centered <- function(df) {
  # Identify significant points
  is_clust <- df$obs > df$hi
  is_disp  <- df$obs < df$lo
  
  # Ranges
  clust_range <- if(any(is_clust)) {
    paste0(round(min(df$r[is_clust]),0), "-", round(max(df$r[is_clust]),0), " m")
  } else "None"
  
  # Determine pattern based on INTENSITY of deviation, not just count
  # Peak deviation value
  max_dev <- max(df$obs)
  max_env <- max(df$hi)
  
  if (any(is_clust) && max_dev > (max_env * 1.1)) {
    dom <- "Clustered (Agregado)"
  } else if (any(is_disp)) {
    dom <- "Regular (Disperso)"
  } else {
    dom <- "Random (Aleatorio)"
  }
  
  data.frame(
    Region = unique(df$region),
    Pattern = dom,
    Clustering_Range_m = clust_range
  )
}

if (length(k_func_results) > 0) {
  l_table <- do.call(rbind, lapply(k_func_results, summarize_l_centered))
  cat("\n### Table. Spatial Pattern Analysis (L-Function)\n")
  print(knitr::kable(l_table, caption = "Significant spatial patterns identified by deviation from CSR envelopes."))
}

cat("\n✓ Spatial analysis complete.\n")
```

# 11. Spatial Segregation by Moran's I (Manuscript Fig. 8)
```{r}
# 
K_ADAPT     <- 12
P_CUTOFF    <- 0.25
USE_FDR     <- FALSE
DO_BUFFER_M <- 0

# 
SIG_EDGE_LWD <- 0.18
NS_EDGE_LWD  <- 0.00
FILL_ALPHA   <- 0.98
NS_ALPHA     <- 0.35
NS_FILL      <- "gray90"

# 
plot_tags <- c(RIO_FRIO = "A", FUQUENE = "B", PARITA = "C", TONOSI = "D")

# 
segregation_results <- list()

# 
for (region_name in names(all_data)) {
  dat <- all_data[[region_name]]
  coords <- as.matrix(dat$sites_df[, c("X", "Y")])
  n <- nrow(coords)
  
  if (n < 5) {
    segregation_results[[region_name]] <- list(stats = data.frame(classification = "Insufficient data"))
    cat(" Insufficient sites for local Moran analysis in", dat$name, "\n")
    next
  }
  
  k_adapt <- min(K_ADAPT, n - 1)
  kn   <- spdep::knearneigh(coords, k = k_adapt)
  dmax <- max(kn$nn.dist)
  if (!is.finite(dmax) || dmax <= 0) {
    nb <- spdep::knn2nb(kn)
  } else {
    nb <- spdep::dnearneigh(coords, 0, dmax * 1.001)
    if (any(spdep::card(nb) == 0)) nb <- spdep::knn2nb(kn)
  }
  lw <- spdep::nb2listw(nb, style = "W", zero.policy = TRUE)
  
  x <- scale(log1p(dat$sites_df$area_ha))[, 1]
  
  lm_out  <- spdep::localmoran(x, lw, zero.policy = TRUE)
  z_moran <- lm_out[, "Z.Ii"]
  
  pval <- 2 * pnorm(abs(z_moran), lower.tail = FALSE)
  if (USE_FDR) pval <- p.adjust(pval, method = "BH")
  
  Wx  <- spdep::lag.listw(lw, x, zero.policy = TRUE)
  sig <- pval < P_CUTOFF
  
  cl <- rep("Not significant", n)
  cl[sig & x > 0 & Wx > 0] <- sprintf("High–High (p<%.2f)", P_CUTOFF)
  cl[sig & x < 0 & Wx < 0] <- sprintf("Low–Low (p<%.2f)",  P_CUTOFF)
  cl[sig & x > 0 & Wx < 0] <- "High–Low (outlier)"
  cl[sig & x < 0 & Wx > 0] <- "Low–High (outlier)"
  
  segregation_results[[region_name]] <- list(
    stats = data.frame(
      site_id = dat$sites_df$site_id,
      moran_I = lm_out[, "Ii"],
      z_score = z_moran,
      p_val   = pval,
      Wx = Wx, x = x,
      classification = cl,
      X = dat$sites_df$X, Y = dat$sites_df$Y,
      region = dat$name
    )
  )
  
  cat(" ", dat$name, "— HH:",
      sum(grepl("^High–High", cl)), "| LL:",
      sum(grepl("^Low–Low", cl)), "\n")
}

# 
lab_p <- sprintf("p<%.2f", P_CUTOFF)
moran_cols <- setNames(
  c("#d73027", "#4575b4"),
  c(paste0("High–High (", lab_p, ")"),
    paste0("Low–Low (",  lab_p, ")"))
)

# 
moran_plot_list    <- list()
moran_summary_list <- list()

# 
for (region_name in names(all_data)) {
  dat <- all_data[[region_name]]
  tag_lab <- plot_tags[[region_name]]
  
  if (is.null(segregation_results[[region_name]]) ||
      is.null(segregation_results[[region_name]]$stats)) {
    p_empty <- ggplot() +
      geom_sf(data = dat$boundary_mask, fill = NA, color = "black", linewidth = 0.8) +
      annotate("text", x = Inf, y = Inf, label = "Insufficient data",
               hjust = 1.1, vjust = 1.2, size = 4, color = "gray40") +
      labs(title = dat$name, subtitle = "Insufficient data", tag = tag_lab) +
      coord_sf(datum = NA, expand = FALSE) +
      (if (exists("base_theme_map")) base_theme_map else theme_minimal()) +
      theme(plot.tag = element_text(face = "bold", size = 16))
    moran_plot_list[[region_name]] <- p_empty
    next
  }
  
  crs_use   <- sf::st_crs(dat$boundary_mask)
  stats_reg <- segregation_results[[region_name]]$stats
  sites_polys <- sf::st_transform(dat$sites_polygons, crs_use)
  
  if ("site_id" %in% names(sites_polys) && "site_id" %in% names(stats_reg)) {
    sites_polys <- dplyr::left_join(
      sites_polys, stats_reg[, c("site_id", "classification")], by = "site_id"
    )
  } else {
    pts_reg <- sf::st_as_sf(stats_reg, coords = c("X","Y"), crs = crs_use)
    sites_polys <- sf::st_join(sites_polys, pts_reg["classification"], left = TRUE, largest = TRUE)
  }
  
  lvls <- names(moran_cols)
  sites_polys$classification <- ifelse(
    is.na(sites_polys$classification), "Not significant", sites_polys$classification
  )
  sites_sig <- dplyr::filter(sites_polys, classification %in% lvls)
  sites_ns  <- dplyr::filter(sites_polys, !classification %in% lvls)
  
  tot_n  <- nrow(sites_polys)
  hh_n   <- nrow(dplyr::filter(sites_sig, classification == lvls[1]))
  ll_n   <- nrow(dplyr::filter(sites_sig, classification == lvls[2]))
  ns_n   <- tot_n - hh_n - ll_n
  
  prop_text <- paste0(
    "Hot: ",  scales::percent(hh_n / tot_n, accuracy = 1),
    " | Cold: ", scales::percent(ll_n / tot_n, accuracy = 1),
    " | NS: ",   scales::percent(ns_n / tot_n, accuracy = 1)
  )
  
  p_map <- ggplot() +
    geom_sf(data = dat$boundary_mask, fill = NA, color = "black", linewidth = 0.8) +
    { if (nrow(sites_ns) > 0)
      geom_sf(data = sites_ns, fill = NS_FILL, color = NA,
              linewidth = NS_EDGE_LWD, alpha = NS_ALPHA) } +
    { if (nrow(sites_sig) > 0)
      geom_sf(data = sites_sig, aes(fill = factor(classification, levels = lvls)),
              color = "black", linewidth = SIG_EDGE_LWD, alpha = FILL_ALPHA) } +
    ggspatial::annotation_north_arrow(
      location = "tr", which_north = "true",
      style = ggspatial::north_arrow_fancy_orienteering
    ) +
    ggspatial::annotation_scale(location = "bl", width_hint = 0.35) +
    scale_fill_manual(values = moran_cols, name = "Local Moran", drop = FALSE) +
    labs(title = dat$name, subtitle = prop_text, tag = tag_lab) +
    coord_sf(datum = NA, expand = FALSE) +
    (if (exists("base_theme_map")) base_theme_map else theme_minimal()) +
    theme(
      legend.position = "right",
      legend.title = element_text(size = 10),
      legend.text  = element_text(size = 9),
      plot.title   = element_text(face = "bold", size = 14),
      plot.subtitle= element_text(size = 10),
      plot.tag     = element_text(face = "bold", size = 16)
    )
  
  moran_plot_list[[region_name]] <- p_map
  
  moran_summary_list[[region_name]] <- data.frame(
    Region = dat$name,
    Hot  = scales::percent(hh_n / tot_n, 1),
    Cold = scales::percent(ll_n / tot_n, 1),
    NS   = scales::percent(ns_n / tot_n, 1)
  )
}

# 
combined_moran_plot <- (
  moran_plot_list[["RIO_FRIO"]] | moran_plot_list[["FUQUENE"]]
) / (
  moran_plot_list[["PARITA"]]   | moran_plot_list[["TONOSI"]]
)

# 
print(
  combined_moran_plot +
    patchwork::plot_layout(guides = "collect") &
    theme(legend.position = "right")
)

# 
cat("\n\n### Summary: Local Moran (lax p, no FDR; HH/LL emphasized)\n\n")
moran_summary_df <- dplyr::bind_rows(moran_summary_list)
print(knitr::kable(moran_summary_df, align = "lccc"))
cat("\n✓ Maps without halos; HH/LL with thin black outline; others in gray.\n")
```

# 12. Spatial Hierarchy (Mantel Test)
```{r}
# 
mantel_results <- data.frame()

# 
for (region_name in names(all_data)) {
  data   <- all_data[[region_name]]
  coords <- as.matrix(data$sites_df[, c("X", "Y")])
  areas  <- data$sites_df$area_ha

  if (nrow(coords) < 5) {
    cat("Region:", data$name, "- Insufficient sites for Mantel test\n")
    next
  }

  dist_geo  <- dist(coords)
  dist_area <- dist(areas)

  mantel_test <- vegan::mantel(dist_geo, dist_area, method = "spearman", permutations = 10000)

  mantel_results <- rbind(mantel_results, data.frame(
    region   = data$name,
    mantel_r = mantel_test$statistic,
    p_value  = mantel_test$signif
  ))
}

# 
cat("\n\n### Table (Adapted): Mantel Test Results (Geo Distance vs Area Distance)\n\n")
print(knitr::kable(mantel_results, digits = 4, row.names = FALSE))
cat("\n✓ Mantel test analysis complete\n\n")
```

# 13. Comparative Synthesis
## 13.1. Master Summary Table
```{r}
# 13. Comparative Synthesis
## 13.1. Master Summary Table
# -----------------------------------------------------------------------------
# DYNAMIC TABLE GENERATION
# Extracts values from previously calculated list objects to ensure data consistency
# -----------------------------------------------------------------------------

# --- GENERATE TABLE 1: DATASET VALIDATION ---
if (exists("dataset_validation_list") && length(dataset_validation_list) > 0) {
  Table_1 <- do.call(rbind, dataset_validation_list)
  colnames(Table_1) <- c("Region", "N", "Edge (%)", "95% CI", "Centroid Risk", 
                         "Slope (°)", "ρ (area-slope)", "p", "CV (area)")
} else {
  Table_1 <- data.frame(Message = "Run Section 2 to generate validation data")
}

# --- GENERATE TABLE 2: NETWORK ANALYSIS ---
if (exists("network_summary_list") && length(network_summary_list) > 0) {
  Table_2 <- do.call(rbind, network_summary_list)
  
  desired_names <- c("Region", "Network Type", "Community Method", "K Neighbors", 
                     "Communities Found k", "Modularity Network", "Total Edges", "Mean Betweenness")
  
  colnames(Table_2) <- desired_names
  Table_2 <- Table_2[, desired_names]
} else {
  Table_2 <- data.frame(Message = "Run Section 6 to generate network data")
}

# --- GENERATE TABLE 3: CONNECTIVITY ---
if (exists("crit_table") && length(crit_table) > 0) {
  Table_3 <- do.call(rbind, crit_table)
  
  # Calculate Median Distance
  median_dists <- sapply(names(all_data), function(rn) {
    coords <- st_coordinates(all_data[[rn]]$sites_centroids)
    if(nrow(coords) > 1) return(round(median(dist(coords)), 0)) else return(NA)
  })
  
  region_names_map <- sapply(all_data, function(x) x$name)
  names(median_dists) <- region_names_map
  
  if("region" %in% colnames(Table_3)) {
    Table_3$`Median Distance (m)` <- median_dists[as.character(Table_3$region)]
    Table_3 <- Table_3[, c("region", "Median Distance (m)", "crit_m", "crit_prop")]
    colnames(Table_3) <- c("Region", "Median Distance (m)", "Critical Threshold (m)", "Proportion at Threshold")
  }
} else {
  Table_3 <- data.frame(Message = "Run Section 7 to generate connectivity data")
}

# --- GENERATE TABLE 4: CONVERGENCE (ARI) ---
if (exists("net_comm_table") && is.data.frame(net_comm_table)) {
  Table_4 <- net_comm_table
  cols_to_keep <- c("region", "n_sites", "leiden_k", "leiden_resolution", 
                    "hdbscan_k", "idw_k", 
                    "ARI_Leiden_HDBSCAN", "ARI_Leiden_IDW", "ARI_IDW_HDBSCAN")
  
  if(all(cols_to_keep %in% colnames(Table_4))) {
    Table_4 <- Table_4[, cols_to_keep]
    colnames(Table_4) <- c("Region", "n sites", "Leiden k", "Leiden resolution", 
                           "HDBSCAN k", "IDW k", 
                           "ARI Leiden - HDBSCAN", "ARI Leiden - IDW", "ARI IDW - HDBSCAN")
  }
} else {
  Table_4 <- data.frame(Message = "Run Section 8 to generate ARI data")
}

# -----------------------------------------------------------------------------
# PRINT TABLES
# -----------------------------------------------------------------------------

cat("### Table 1. Dataset Validation\n")
print(knitr::kable(Table_1, row.names = FALSE, digits = 3))
cat("\n\n")

cat("### Table 2. Network Analysis Summary (k-NN)\n")
print(knitr::kable(Table_2, row.names = FALSE, digits = 4))
cat("\n\n")

cat("### Table 3. Critical Spatial Connectivity Thresholds\n")
print(knitr::kable(Table_3, row.names = FALSE, digits = 1))
cat("\n\n")

cat("### Table 4. Convergence of Models (Adjusted Rand Index)\n")
print(knitr::kable(Table_4, row.names = FALSE, digits = 3))
cat("\n")
```